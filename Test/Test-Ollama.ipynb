{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "594df057",
   "metadata": {},
   "source": [
    "# Ollama API Testing\n",
    "\n",
    "This notebook tests the connection to the Ollama container and verifies that the llama3.2 model is working properly.\n",
    "\n",
    "## Steps:\n",
    "1. Test basic connectivity to Ollama\n",
    "2. Import required libraries  \n",
    "3. Set up API constants\n",
    "4. Check if model is available (and pull if needed)\n",
    "5. Test the model with a simple chat message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4218c52e-6d90-422c-9b94-924479e83ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import requests\n",
    "\n",
    "response = requests.get(\"http://ollama:11434/api/tags\")\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018a392c-8f5c-4738-b054-fab9e2906125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d02b0df-7dd8-4f9a-b8b2-006d160b0339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "OLLAMA_API = \"http://ollama:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba592f1c-32c0-46fd-bf54-e12cf73bb59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test chatting with the LLM\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello! Can you tell me a short joke about programming?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "payload = {\n",
    "    \"model\": MODEL,\n",
    "    \"messages\": messages,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "print(\"Sending message to llama3.2...\")\n",
    "try:\n",
    "    response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(\"\\nü§ñ Response from llama3.2:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(result['message']['content'])\n",
    "        print(\"-\" * 40)\n",
    "    else:\n",
    "        print(f\"‚ùå API call failed with status: {response.status_code}\")\n",
    "        print(f\"Response: {response.text}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error making API call: {e}\")\n",
    "    print(\"Make sure the Docker containers are running!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1eedd6-9bcd-45f8-bca4-b255778ac0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if model is available and pull if needed\n",
    "print(\"Checking if llama3.2 model is available...\")\n",
    "\n",
    "try:\n",
    "    response = requests.get(\"http://ollama:11434/api/tags\")\n",
    "    if response.status_code == 200:\n",
    "        models = response.json()\n",
    "        model_names = [model['name'] for model in models.get('models', [])]\n",
    "        print(f\"üìã Available models: {model_names}\")\n",
    "        \n",
    "        # Check if llama3.2 is available (could be 'llama3.2:latest' or just 'llama3.2')\n",
    "        llama_available = any('llama3.2' in name for name in model_names)\n",
    "        \n",
    "        if not llama_available:\n",
    "            print(\"‚¨áÔ∏è Model llama3.2 not found. Pulling model...\")\n",
    "            # Pull the model using the API\n",
    "            pull_response = requests.post(\"http://ollama:11434/api/pull\", \n",
    "                                        json={\"name\": \"llama3.2\"})\n",
    "            if pull_response.status_code == 200:\n",
    "                print(\"‚úÖ Model pulled successfully!\")\n",
    "            else:\n",
    "                print(f\"‚ùå Failed to pull model: {pull_response.status_code}\")\n",
    "        else:\n",
    "            print(\"‚úÖ Model llama3.2 is already available!\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to check models: {response.status_code}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error checking models: {e}\")\n",
    "    print(\"Make sure the Docker containers are running!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d23be7-ffea-43b3-9996-fae22741e276",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
